## 为手机的T9输入法设计一个索引
提供一个包含所有英文单词的字典，为手机的T9输入法设计一个索引，例如输入4能够提示出g、h、i开头的英文单词（greate、hello、...），输入43能够提示出ge、he、id、if (hello...) 等词开通的英文单词。
思路：
使用trie树存储索引，但是如果使用普通的以字母形式组织的trie树检索非常麻烦。因此需要先将所有的英文单词改写成数字，例如：
Greate:473283
Hello:43556
然后组织成数字的trie树，所有的单词都挂在这棵trie树上。

## 这是一套题目的3个子问题，难度依次递进：
问题1:描述快排的基本思想并进行编码，以及一个典型的应用。
问题2:快排可以应用在链表上么？
问题3:除了快排，还有其他的排序可以在链表上达到O(lnN)的复杂度么？
解答1:典型应用：求第N大或者第N小的数。
解答2:`快排可以应用在链表上`。每次迭代需要两个指针，一个指向比pivot大的结点的链表，一个指向比pivot小的结点的链表，然后合并。
解答3:
还可以使用归并排序，逻辑比较复杂，需要考虑临时指针的使用，每个临时指针在过程中多次复用，否则会消耗大量的空间。具体参考stl中list的排序算法。

## 颜色变换智力题
有一个8*8的矩形，分成64个1*1的小方块。每个方块要么染成黑色或者白色。现在存在两种颜色互换操作：
第一种是将一个任意3*3的矩形里面的所有小方块的颜色互换（即黑变白，白变黑）；
第二种是将一个任意4*4的矩形里面的所有小方块的颜色互换；
那么对于任意一种染色方案，是否都可以通过这两种颜色互换操作（可以多次操作）将所有的64个小方块的颜色都变成白色？
思路：
显然，对于初始的矩形而言，一共有2^64种染色方案。第一种颜色互换操作一共有（8-3+1）*(8-3+1)共36种小类型，第二种颜色互换操作一共有5*5=25种类型，一共有61种小类型。由于在颜色互换的过程时，每种小类型最多出现一次（两次相同小类型的操作相当于没有操作，颜色没有变化），且最终结果与操作的顺序无关。所以颜色互换操作的状态最多只有2^61种，是小于2^64种染色方案的。因此，最终的答案是不可能的。


## 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。
方案1：采用`2-Bitmap`（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。
   
方案2：也可采用划分小文件的方法，然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。

`BitMap算法`：来自于《编程珠玑》。所谓的Bit-map就是用一个bit位来标记某个元素对应的Value， 而Key即是该元素。由于采用了Bit为单位来存储数据，因此在存储空间方面，可以大大节省。 
假设我们要对0-7内的5个元素(4,7,2,5,3)排序（这里假设这些元素没有重复）。那么我们就可以采用Bit-map的方法来达到排序的目的。要表示8个数，我们就只需要8个Bit（1Bytes），首先我们开辟1Byte的空间，将这些空间的所有Bit位都置为0 然后遍历这5个元素，首先第一个元素是4，那么就把4对应的位置为1（可以这样操作 p+(i/8)|(0×01<<(i%8)) 当然了这里的操作涉及到Big-ending和Little-ending的情况，这里默认为Big-ending）,因为是从零开始的，所以要把第五位置为1。 
然后再处理第二个元素7，将第八位置为1,，接着再处理第三个元素，一直到最后处理完所有的元素，将相应的位置为1。 
然后我们现在遍历一遍Bit区域，将该位是一的位的编号输出（2，3，4，5，7），这样就达到了排序的目的。


## 输入一个整数数组，调整数组中数字的顺序，使得所有奇数位于数组的前半部分，所有偶数位于数组的后半部分，要求时间复杂度为O(n)
维护两个指针，第一个指针指向数组的第一个数字，只向后移动，第二个指针指向最后一个数字，只向前移动。当第一个指针指向数字为偶数，且第二个指针指向数字为奇数时，交换数字，并移动两个指针。


## 说出你所知道的5种排序算法，并分析时间空间复杂度和各自的稳定性
![image](https://github.com/woojean/woojean.github.io/blob/master/images/algo.png)


## 100w个数中找出最大的100个数。
方案1：用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。
方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，直到比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w*100)。
方案3：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。


## 有5瓶药，药片形状大小一样，每瓶数量一样。其中一瓶是坏的，每片9mg，好的药片每片10mg。需要一次区分哪瓶是坏药，怎么做？
1.你有一个高灵敏度的天平，有任意规格砝码
给5个瓶依次编号，分别取出1、2、3、4、5片药，根据砝码不同的质量区分坏药瓶的序号

2.你有一个高灵敏度的天平，但没有砝码
给5个瓶依次编号，第5瓶放开不取，从1~4瓶中分别取出100、1、100、1片药，放置1、2和3、4分别于天平左右侧:
（1）看是否平衡，若平衡，5是坏的；如不平衡；
（2）左侧低右侧高，看灵敏天平的偏转角度，大3，小4；
（3）左侧高右侧低，看灵敏天平的偏转角度，大1，小2。


## 跨域登录同步
用户在bbb.com上已经登陆，现在要去aaa.com上玩，但在aaa.com域名下暂未登录。需要访问的aaa.com/resource.html资源需要登录才能访问。两个网站是同一套会员体系，同一个公司的。这时要让用户体验上做到用户在aaa.com上玩也能识别出登录状态（而不是要登录2次）。
第一步：用户向aaa.com发起get请求，获取resource.html资源，aaa.com发现用户未登录，返回302状态和外部重定向url:
j.bbb.com?target=www.aaa.com/resource.html，注意j.bbb.com子域名上部署的应用可以认为是专门用于跨域同步。
第二步：用户根据重定向url，访问
j.bbb.com?target=www.aaa.com/resource.html，`由于在bbb.com上已经登录，所以bbb.com上能拿到从client端传递过来cookie信息`。子域j.bbb.com上的应用负责`将cookie读取出来，并作为参数再次重定向`到
p.aaa.com?tartet=www.aaa.com/resource.html&sessionid=xxx&loginId=xxx&……  
第三步：用户根据第二步重定向url，访问p.aaa.com。p.aaa.com子域名（可以理解为该子域名用来处理aaa.com域名下的登录逻辑）上的应用专门负责`根据请求参数里的参数对，往aaa.com域写入cookie（改变登录状态）`，并重定向到用户第一步请求的url。
第四步：经过前三步，已经完成了在aaa.com域名下同步bbb.com的登录状态，用户再次请求aaa.com/resource.html，这时就能成功访问了。

## 输入一个英文句子，翻转句子中单词的顺序，但单词内字符的顺序不变。
句子中单词以空格符隔开。为简单起见，标点符号和普通字母一样处理。
例如输入"I am a student."，则输出"student. a am I"。
思路：
先颠倒句子中的所有字符，再颠倒每个单词内的字符。
如例句中，"I am a student."第一次整体翻转后得到".tneduts a ma I"；
第二次在每个单词中内部翻转得到"students. a am I"，即为题解。


## 撞球问题
在水平光滑无限长的管道两端，有若干相向而行的小球，球的直径和管的直径相同，所有球的质量和速率均相同，球与球之间有一定间距。当两个相向而行的小球相遇时，发生完全弹性碰撞，即为一次碰撞。求下列情况下的碰撞总次数
（1）两端各3个球；
（2）一端6个球，一端3个球；
（3）一端M个球，一端N个球
答案：
（1）9；
（2）18；
（3）M*N
思路:
对于（1）或（2），可以用穷举法计算，目的是引导推导出M*N的规律
（2）可以从（1）中推导，即将（2）分成两组（1）来考虑
（3）有两种思路，简单的，可以考虑将（2）的分组考虑成18个1*1组合，类推出M*N；另一种思路，当发生一次碰撞时，可以看成两个小球碰撞后交换位置。


## 尽可能的优化代码性能
如下代码成为了系统的瓶颈，请尽可能的优化；要求找到优化的点和优化方案，分析说明原因。
代码如下：
```
#define  M   10000
#define  N   10000
#define  L    3
int arr[L][M][N];
int xxx[M * N];
int main()
{
  // init arr and xxx first, omit
  // start
  for (int r=0; r<10000; ++r){
    f(arr);
  }
  return 0;
}

void f( int arr[L][M][N] )
{
  int k = 0;
  for ( int m=0; m<M; m++ ){
    for( int n=0; n<N; n++ ){
        for ( int l=0; l<L; l++){
            int ss = arr[l][m][0] + 1111;
            int tmp = sss + power( log( arr[l][m][n] ), 3 );
            arr[l][m][n] = a[l][m][n] + tmp;
            xxx[k] += arr[l][m][n];
        }
      k = k + 1;
    }
  }
}
```
修改点：
1.power函数，可以直接写成a*a*a，这个优化效果与机器型号有关
2.改变数据结构，arr[L][M][N]修改成arr[M][N][L]，修改后的好处：
   a) 内层循环的一些共有计算可以提前
   b) 内层数据访问被连续存储，cache命中率极度提升（最重要也是最根本的优化点）
3.改变循环方式，for l, for m, for n；配合2，修改循环方式后，可以对l循环进行循环展开，减少分支预测失败
4.….



## 如何对递归程序进行时间复杂度分析？
例子：求N!。 这是一个简单的"累乘"问题，用递归算法也能解决。 
  n! = n * (n - 1)!   n > 1 
  0! = 1, 1! = 1      n = 0,1 
因此，递归算法如下： 
```  
  fact(int n) {  
    if(n == 0 || n == 1)       		
      return 1;  
    else   
      return n * fact(n - 1);  
  }
```    
以n=3为例，看运行过程如下： 
    fact(3) ----- fact(2) ----- fact(1) ------ fact(2) -----fact(3) 
    ------------------------------>  ------------------------------> 
                递归                            回溯 
  递归算法在运行中不断调用自身降低规模的过程，当规模降为1，即递归到fact(1)时，满足停止条件停止递归，开始回溯(返回调用算法)并计算，从fact(1)=1计算返回到fact(2);计算2*fact(1)=2返回到fact(3)；计算3*fact(2)=6，结束递归。
递归算法的分析方法比较多，最常用的便是迭代法。 
  迭代法的基本步骤是先将递归算法简化为对应的递归方程，然后通过反复迭代，将递归方程的右端变换成一个级数，最后求级数的和，再估计和的渐进阶。 
  	<1> 例：n! 
       算法的递归方程为： T(n) = T(n - 1) + O(1); 
       迭代展开： T(n) = T(n - 1) + O(1) 
                       = T(n - 2) + O(1) + O(1) 
                       = T(n - 3) + O(1) + O(1) + O(1) 
                       = ...... 
                       = O(1) + ... + O(1) + O(1) + O(1) 
                       = n * O(1) 
                       = O(n) 
      这个例子的时间复杂性是线性的。 
	<2> 例：如下递归方程： 
      T(n) = 2T(n/2) + 2, 且假设n=2的k次方。 
      T(n) = 2T(n/2) + 2 
           = 2(2T(n/2*2) + 2) + 2 
           = 4T(n/2*2) + 4 + 2 
           = 4(2T(n/2*2*2) + 2) + 4 + 2 
           = 2*2*2T(n/2*2*2) + 8 + 4 + 2 
           = ... 
           = 2的(k-1)次方 * T(n/2的(i-1)次方) + $(i:1~(k-1))2的i次方 
           = 2的(k-1)次方 + (2的k次方)  - 2 
           = (3/2) * (2的k次方) - 2 
           = (3/2) * n - 2 
           = O(n) 
      这个例子的时间复杂性也是线性的。 
	<3> 例：如下递归方程： 
      T(n) = 2T(n/2) + O(n), 且假设n=2的k次方。 
      T(n) = 2T(n/2) + O(n) 
           = 2T(n/4) + 2O(n/2) + O(n) 
           = ... 
           = O(n) + O(n) + ... + O(n) + O(n) + O(n) 
           = k * O(n) 
           = O(k*n) 
           = O(nlog2n) //以2为底 
     
      一般地，当递归方程为`T(n) = aT(n/c) + O(n)`, T(n)的解为： 
      O(n)          (a<c && c>1) 
      O(nlog2n)     (a=c && c>1) //以2为底 
      O(nlogca)     (a>c && c>1) //n的(logca)次方，以c为底 
   上面介绍的3种递归调用形式，比较常用的是第一种情况，第二种形式也有时出现，而第三种形式(间接递归调用)使用的较少，且算法分析比较复杂。下面举个第二种形式的递归调用例子。 
  	<4> 递归方程为：T(n) = T(n/3) + T(2n/3) + n 
     为了更好的理解，先画出递归过程相应的递归树： 
                            n                        --------> n 
                    n/3            2n/3              --------> n 
              n/9       2n/9   2n/9     4n/9         --------> n 
           ......     ......  ......  .......        ...... 
                                                     -------- 
                                                     总共O(nlogn) 
     累计递归树各层的非递归项的值，每一层和都等于n，从根到叶的最长路径是： 
      n --> (2/3)n --> (4/9)n --> (12/27)n --> ... --> 1 
     设最长路径为k，则应该有：(2/3)的k次方 * n = 1 
     得到 k = log(2/3)n  // 以(2/3)为底 
     于是 T(n) <= (K + 1) * n = n (log(2/3)n + 1) 
     即 T(n) = O(nlogn) 
   由此例子表明，对于第二种递归形式调用，借助于递归树，用迭代法进行算法分析是简单易行的。


## 有N个球，其中只有一个是重量较轻的，用天平只称三次就能找到较轻的球，以下的N值哪个是可能的？ 
A 12
B 16
C 20
D 24
E 28
3 个一次可以测出来，3*3 = 9 个以内 2 次，3*3*3 = 27 个以内，3次！


## 现在平面上n条直线，n条直线将平面划分为若干个区域，现在用若干种颜色对这些区域染色，那么至少需要多少种颜色，才能使相邻区域（两个区域的公共部分只有一个点不能视为相邻）的颜色不同？
答案是两种（无论n条直线将平面划分后是怎样的）。首先是考虑一条直线的情况，显然可以用黑白这两种颜色就能满足要求。当已经将n-1条直线划分的区域染成黑白两种颜色时，那么增加一条新的直线时候，将直线左边的区域的颜色全部从黑变白，白变黑。而右边的区域的颜色保持不变，那么最终相邻区域的颜色还是保持不同。


## 有一架天平，左右两侧均可以放砝码。现在希望能用它称量出从1克开始，尽可能多的连续的整数质量。如果现在允许你任意选择砝码组合，那么对于N个砝码来说最多可以称量出多少个从1克开始的连续质量？
(3N-1)/2，即1+3+32+…+3N-1。砝码的组合方式是使用1克，3克，9克，…，3N-1克的砝码各一个。例如N=2时，称量出1~4克的方法是：
1 = 1; 
2 = 3-1;
3 = 3;
4 = 3+1;
而N=3时，依此不难得到5~13克的构造方法。形式化的证明可以使用数学归纳法。
最可能的得到上述答案的思路是从N较小的情况入手，使用递推方法，发现每增加一个砝码时应选择3N-1克的质量。随后使用数学归纳法证明。


## 秒杀类系统架构优化思路
将请求尽量拦截在系统上游，并且充分利用缓存。由上游至低层优化如下：
1.前端（浏览器、APP）
控制实际往后端发送请求的数量，如用户点击“查询”后，将按钮置灰，禁止用户在短时间内重复提交。

2.站点层（访问后端数据，拼写html返回）
对uid进行请求计数和去重，比如5秒内只准透过一个请求（可以使用redis设置过期时间实现）。缺点是当有多台机器时（此时相当于5s内限制n个访问），数据可能不准（脏读,但数据库层面真实数据是没问题的）。
假设有海量真实的对站点层的请求，可以通过增加机器来扩容，实在不行只能抛弃部分请求（返回稍后再试），原则是要保护系统，不能让所有用户都失败；

3.服务层（提供数据访问）
对于读请求，使用缓存。
对于写请求，使用请求队列（队列成本很低），每次只透有限的写请求（如总票数）去数据层，如果均成功，再放下一批。可以不用统一一个队列，这样的话每个服务透过更少量的请求（总票数/服务个数），这样简单。统一一个队列又复杂了。对于失败的处理无需重放，返回用户查询失败或者下单失败，架构设计原则之一是“fail fast”。

4.数据层（数据库、缓存）
经过以上步骤，到数据库层的请求已经有限。

此外还可以做一些业务规则上的优化，如：12306分时分段售票、数据粒度优化（如只展示有、无，而不是具体的数量）、业务逻辑异步（先创建订单，但是状态为未支付，如果超时仍未支付，则恢复库存）。


## 给定一棵二叉树的前序和中序遍历序列，重建这个二叉树
如先序为 124356， 中序为 421536
1．首先要面试者正确写出一个二叉树的先序和中序序列
2．然后询问根据先序和中序结果，是否可以重建出这棵二叉树？看面试者是否能够意识到二叉树节点内容相同的情况。
3．简化问题，假设二叉树节点只使用标号表示，没有重复，写程序根据先序和中序序列重构出二叉树。
4．分析：
先序中的第一个节点1是二叉树的根，找到该节点在中序中的位置，则1之前的为二叉树的左子树（42），1之后的为二叉树的右子树（536），然后根据先序中的对应子树24和356，递归重建即可。
重建二叉树：先序+中序（可以），后序+中序（可以），`先序+后序（不可以）`


## 海量日志数据，提取出某日访问百度次数最多的那个IP。
首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如模1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map进行频率统计，然后再找出频率最大的几个）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。
算法思想：`分而治之+Hash`
1.IP地址最多有2^32=4G种取值情况，所以不能完全加载到内存中处理； 
2.可以考虑采用“分而治之”的思想，按照IP地址的Hash(IP)%1024值，把海量IP日志分别存储到1024个小文件中。这样，每个小文件最多包含4MB个IP地址； 
3.对于每一个小文件，可以构建一个IP为key，出现次数为value的Hash map，同时记录当前出现次数最多的那个IP地址；
4.可以得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP；


## 说明链表和数组作为数据的不同组织形式，各自的优缺点。
数组，在内存上给出了连续的空间。链表，内存地址上可以是不连续的，每个链表的节点包括原来的内存和下一个节点的信息(单向的一个，双向链表的话，会有两个)。
数组优于链表的:
1.内存空间占用的少，因为链表节点会附加上一块或两块下一个节点的信息。
　　但是数组在建立时就固定了。所以也有可能会因为建立的数组过大或不足引起内存上的问题。
2.数组内的数据可随机访问，但链表不具备随机访问性。这个很容易理解，数组在内存里是连续的空间，比如如果一个数组地址从100到200，且每个元素占用两个字节，那么100-200之间的任何一个偶数都是数组元素的地址，可以直接访问。
　　链表在内存地址可能是分散的。所以必须通过上一节点中的信息找能找到下一个节点。
3.查找速度。

链表优于数组的:
1. 插入与删除的操作。如果数组的中间插入一个元素，那么这个元素后的所有元素的内存地址都要往后移动。删除的话同理。只有对数据的最后一个元素进行插入删除操作时，才比较快。链表只需要更改有必要更改的节点内的节点信息就够了。并不需要更改节点的内存地址。
2.内存地址的利用率方面。不管你内存里还有多少空间，如果没办法一次性给出数组所需的要空间，那就会提示内存不足，磁盘空间整理的原因之一在这里。而链表可以是分散的空间地址。
3.链表的扩展性比数组好。因为一个数组建立后所占用的空间大小就是固定的，如果满了就没法扩展，只能新建一个更大空间的数组;而链表不是固定的，可以很方便的扩展。


## 什么是依赖注入？和控制反转是什么关系？
IoC（Inversion of Control）控制反转
DI（Dependency Injection）依赖注入

DI是IoC的一种具体实现，另一种主要的实现方式是服务定位器（Service Locator）。

没有IoC的时候，常规的A类使用C类的示意图：
 ![image](https://github.com/woojean/woojean.github.io/blob/master/images/img_1.png)

有IoC的时候，A类不再主动去创建C，而是被动等待，等待IoC的容器获取一个C的实例，然后反向地注入到A类中。
 ![image](https://github.com/woojean/woojean.github.io/blob/master/images/img_2.png)


## 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。
方案：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。
如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。


## 依赖、关联、聚合、组合的区别。
`依赖：Uses a。这种使用关系是具有偶然性的、临时性的、非常弱的，但是B类的变化会影响到A；比如类B作为参数被类A在某个method方法中使用；

`关联`：Has a。这种关系比依赖更强、不存在依赖关系的偶然性、关系也不是临时性的，一般是长期性的，而且双方的关系一般是平等的、关联可以是单向、双向的；表现在代码层面，为被关联类B以类属性的形式出现在关联类A中，也可能是关联类A引用了一个类型为被关联类B的全局变量；

`聚合`：Own a。聚合是关联关系的一种特例，他体现的是整体与部分、拥有的关系，即has-a的关系，此时整体与部分之间是可分离的，他们可以具有各自的生命周期，部分可以属于多个整体对象，也可以为多个整体对象共享；比如计算机与CPU、公司与员工的关系等；表现在代码层面，和关联关系是一致的，只能从语义级别来区分；

`组合`：is a part of。这种关系比聚合更强，也称为强聚合；他同样体现整体与部分间的关系，但此时整体与部分是不可分的，整体的生命周期结束也就意味着部分的生命周期结束。


## 统计海量数据中的前10个热门数据
搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，`虽然总数是1千万，但如果除去重复后，不超过3百万个`。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。
`外排序`（External sorting）是指能够处理极大量数据的排序算法。通常来说，`外排序处理的数据不能一次装入内存`，只能放在读写较慢的外存储器（通常是硬盘）上。外排序通常采用的是一种`“排序-归并”`的策略。在排序阶段，先读入能放在内存中的数据量，将其排序输出到一个临时文件，依此进行，`将待排序数据组织为多个有序的临时文件`。尔后在归并段阶将这些临时文件组合为一个大的有序文件，也即排序结果。

`外归并排序`：外排序的一个例子是外归并排序（External merge sort），它读入一些能放在内存内的数据量，在内存中排序后输出为一个顺串（即是内部数据有序的临时文件），处理完所有的数据后再进行归并。比如，要对 900 MB 的数据进行排序，但机器上只有 100 MB 的可用内存时，外归并排序按如下方法操作：
	1.读入 100 MB 的数据至内存中，用某种常规方式（如快速排序、堆排序、归并排序等方法）在内存中完成排序。 
	2.将排序完成的数据写入磁盘。 
	3.重复步骤 1 和 2 直到所有的数据都存入了不同的 100 MB 的块（临时文件）中。在这个例子中，有 900 MB 数据，单个临时文件大小为 100 MB，所以会产生 9 个临时文件。 
	4.读入每个临时文件（顺串）的前 10 MB （ = 100 MB / (9 块 + 1)）的数据放入内存中的输入缓冲区，最后的 10 MB 作为输出缓冲区。（实践中，将输入缓冲适当调小，而适当增大输出缓冲区能获得更好的效果。） 
	5.执行`九路归并算法`，将结果输出到输出缓冲区。一旦输出缓冲区满，将缓冲区中的数据写出至目标文件，清空缓冲区。直至所有数据归并完成。

## 统计热门查询
要统计最热门查询，首先就是要统计每个Query出现的次数，然后根据统计结果，找出Top 10。所以我们可以基于这个思路分两步来设计该算法。
即，此问题的解决分为以下俩个步骤：
第一步：Query统计
Query统计有以下俩个方法，可供选择：
1.直接排序法
    首先我们最先想到的的算法就是排序了，首先对这个日志里面的所有Query都进行排序，然后再遍历排好序的Query，统计每个Query出现的次数了。但是题目中有明确要求，那就是内存不能超过1G，一千万条记录，每条记录是255Byte，很显然要占据2.375G内存，这个条件就不满足要求了。
让我们回忆一下数据结构课程上的内容，当数据量比较大而且内存无法装下的时候，我们可以采用外排序的方法来进行排序，这里我们可以采用归并排序，因为归并排序有一个比较好的时间复杂度O(NlgN)。排完序之后我们再对已经有序的Query文件进行遍历，统计每个Query出现的次数，再次写入文件中。综合分析一下，排序的时间复杂度是O(NlgN)，而遍历的时间复杂度是O(N)，因此该算法的总体时间复杂度就是O(N+NlgN)=O（NlgN）。
2.`Hash Table法`
    在第1个方法中，我们采用了排序的办法来统计每个Query出现的次数，时间复杂度是NlgN，那么能不能有更好的方法来存储，而时间复杂度更低呢？
	题目中说明了，`虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query`，每个Query255Byte，因此我们可以考虑把他们都放进内存中去，而现在只是需要一个合适的数据结构，在这里，Hash Table绝对是我们优先的选择，因为`Hash Table的查询速度非常的快，几乎是O(1)的时间复杂度`。
	那么，我们的算法就有了：`维护一个Key为Query字串，Value为该Query出现次数的HashTable，每次读取一个Query，如果该字串不在Table中，那么加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加一即可`。最终我们在O(N)的时间复杂度内完成了对该海量数据的处理。
	本方法相比算法1：在时间复杂度上提高了一个数量级，为O（N），但不仅仅是时间复杂度上的优化，该方法只需要IO数据文件一次，而算法1的IO次数较多的，因此该算法2比算法1在工程上有更好的可操作性。

第二步：找出Top 10
算法一：普通排序
我想对于排序算法大家都已经不陌生了，这里不在赘述，我们要注意的是排序算法的时间复杂度是NlgN，在本题目中，三百万条记录，用1G内存是可以存下的。
算法二：部分排序
题目要求是求出Top 10，因此我们没有必要对所有的Query都进行排序，我们只需要维护一个10个大小的数组，初始化放入10个Query，按照每个Query的统计次数由大到小排序，然后遍历这300万条记录，每读一条记录就和数组最后一个Query对比，如果小于这个Query，那么继续遍历，否则，将数组中最后一条数据淘汰，加入当前的Query。最后当所有的数据都遍历完毕之后，那么这个数组中的10个Query便是我们要找的Top10了。
		不难分析出，这样，算法的最坏时间复杂度是N*K， 其中K是指top多少。
算法三：堆
在算法二中，我们已经将时间复杂度由NlogN优化到NK，不得不说这是一个比较大的改进了，可是有没有更好的办法呢？
分析一下，在算法二中，每次比较完成之后，需要的操作复杂度都是K，因为要把元素插入到一个线性表之中，而且采用的是顺序比较。这里我们注意一下，该数组是有序的，一次我们每次查找的时候可以采用二分的方法查找，这样操作的复杂度就降到了logK，可是，随之而来的问题就是数据移动，因为移动数据次数增多了。不过，这个算法还是比算法二有了改进。
基于以上的分析，我们想想，有没有一种既能快速查找，又能快速移动元素的数据结构呢？回答是肯定的，那就是堆。
`借助堆结构，我们可以在log量级的时间内查找和调整/移动`。因此到这里，我们的算法可以改进为这样，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。
思想与上述算法二一致，只是算法在算法三，我们采用了最小堆这种数据结构代替数组，把查找目标元素的时间复杂度有O（K）降到了O（logK）。
那么这样，采用堆数据结构，算法三，最终的时间复杂度就降到了N*logK，和算法二相比，又有了比较大的改进。

总结：
至此，算法就完全结束了，经过上述第一步、先用Hash表统计每个Query出现的次数，O（N）；然后第二步、采用堆数据结构找出Top 10，N*O（logK）。所以，我们最终的时间复杂度是：O（N） + N'*O（logK）。（N为1000万，N’为300万）。


## 现在共有13个球，这批球重有一个球的质量和其它球的质量不同（轻重未知）。给你一个天平，至多只有三次的称量机会，怎样将那个质量不一样的球找出来？
将13个球分为4球，4球，5球三组。
(1)	第一次称两个4球组，若不想等，则5球组全是标准球。然后就可以用12球类似的方法解决。
1.1 abcd轻。在efgh中取出fgh，替换掉abcd中的bcd。在ijkl中取出jkl，补充到原来fgh的位置。
如果afgh轻，说明答案为a或e。称量ab，如果相等，答案为e；如果不等，答案为a。
如果afgh重，说明答案在fgh中。称量fg，如果相等，答案为h；如果不等，重者为答案。如果一样重，答案在bcd中。称量bc，如果相等，答案为d；如果不等，轻者为答案。
1.2 abcd重。在efgh中取出fgh，替换掉abcd中的bcd。在ijkl中取出jkl，补充到原来fgh的位置。
如果afgh重，答案为a或e。称量ab，如果相等，答案为e；如果不等，答案为a。
如果afgh轻，答案在fgh中。称量fg，如果相等，答案为h；如果不等，轻者为所求。如果一样重，答案在bcd中。称量bc，如果相等，答案为d；如果不等，重者为答案。
(2)	若两个4球组相等，则异常球存在于5球组中。则从两个4球组中任取一个作为标准球。


### 在一个有序数组中，有些元素重复出现。输入一个数值，求此值在数组中重复的次数
思路有两种:
1.upperbound() – lowerbound()
2.使用类似线段树的思想直接统计
iterator lower_bound( const key_type &key ): 返回一个迭代器，`指向键值>= key的第一个元素`。
iterator upper_bound( const key_type &key ):返回一个迭代器，`指向键值> key的第一个元素`。
例如：map中已经插入了1，2，3，4的话，如果lower_bound(2)的话，返回的2，而upper_bound（2）的话，返回的就是3


## 负载均衡的基本算法
`随机`：负载均衡方法随机的把负载分配到各个可用的服务器上，通过随机数生成算法选取一个服务器，然后把连接发送给它。虽然许多均衡产品都支持该算法，但是它的有效性一直受到质疑，除非把服务器的可运行时间看的很重。
`轮询`：轮询算法按顺序把每个新的连接请求分配给下一个服务器，最终把所有请求平分给所有的服务器。轮询算法在大多数情况下都工作的不错，但是如果负载均衡的设备在处理速度、连接速度和内存等方面不是完全均等，那么效果会更好。
`加权轮询`：该算法中，每个机器接受的连接数量是按权重比例分配的。这是对普通轮询算法的改进，比如你可以设定：第三台机器的处理能力是第一台机器的两倍，那么负载均衡器会把两倍的连接数量分配给第3台机器。
`动态轮询`：类似于加权轮询，但是，权重值基于对各个服务器的持续监控，并且不断更新。这是一个动态负载均衡算法，基于服务器的实时性能分析分配连接，比如每个节点的当前连接数或者节点的最快响应时间等。
`最快算法`：最快算法基于所有服务器中的最快响应时间分配连接。该算法在服务器跨不同网络的环境中特别有用。
`最少连接`：系统把新连接分配给当前连接数目最少的服务器。该算法在各个服务器运算能力基本相似的环境中非常有效。
`观察算法`：该算法同时利用最小连接算法和最快算法来实施负载均衡。服务器根据当前的连接数和响应时间得到一个分数，分数较高代表性能较好，会得到更多的连接。
`预判算法`：该算法使用观察算法来计算分数，但是预判算法会分析分数的变化趋势来判断某台服务器的性能正在改善还是降低。具有改善趋势的服务器会得到更多的连接。该算法适用于大多数环境。


## 如何判断一个链表是否存在回路?
给指针加一个标志域，如访问过则置1.当遍历到标志为1的项说明有了回路。
定义2个指针，一快(fast)一慢(slow)，即：从头向后遍历过程中，每循环一次，快指针一次向后移动2个元素，慢指针移动一个元素，每次判断(   fast==slow   ||   slow==fast->nest   ),如果成立，说明慢指针赶上了快指针，则为循环链表，否则，如果有一个指针到达NULL，则为单链表。


## 有一个庞大的字符串数组，然后给你一个单独的字符串，让你从这个数组中查找是否有这个字符串并找到它，你会怎么做？
有一个方法最简单，老老实实从头查到尾，一个一个比较，直到找到为止...
`所谓Hash，一般是一个整数，通过某种算法，可以把一个字符串"压缩" 成一个整数`。当然，无论如何，一个32位整数是无法对应回一个字符串的，但在程序中，两个字符串计算出的Hash值相等的可能非常小。
是不是把第一个算法改进一下，改成逐个比较字符串的Hash值就可以了呢，答案是，远远不够，要想得到最快的算法，就不能进行逐个的比较，通常是构造一个哈希表(Hash Table)来解决问题，哈希表是一个大数组，这个数组的容量根据程序的要求来定义，例如1024，每一个Hash值通过取模运算 (mod) 对应到数组中的一个位置，这样，只要比较这个字符串的哈希值对应的位置有没有被占用，就可以得到最后的结果了，想想这是什么速度？是的，是最快的O(1)。
冲突解决：分离链接法，用链表解决冲突。
`一个好的hash函数`：
```
/*key为一个字符串，nTableLength为哈希表的长度
*该函数得到的hash值分布比较均匀*/
unsigned long getHashIndex( const char *key, int nTableLength )
{
    unsigned long nHash = 0;
    while (*key)
    {
        nHash = (nHash<<5) + *key++;  // nHash = nHash*32 + *key;  key++
    }
    return ( nHash % nTableLength );
}
```

## 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？
方案1：可以估计每个文件的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。
遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,...,a999）中。这样每个小文件的大约为300M。
遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,...,b999）。`这样处理后，所有可能相同的url都在对应的小文件（a0vsb0,a1vsb1,...,a999vsb999）中，不对应的小文件不可能有相同的url`。然后我们只要求出1000对小文件中相同的url即可。
求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set（STL）中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。

方案2：如果允许有一定的错误率，可以使用`Bloom filter`，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。
Bloom filter：Bloom Filter是一种空间效率很高的随机数据结构，它利用位数组很简洁地表示一个集合，并能判断一个元素是否属于这个集合。Bloom Filter的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter通过极少的错误换取了存储空间的极大节省。`Bloom filter 采用的是哈希函数的方法，将一个元素映射到一个 m 长度的阵列上的一个点，当这个点是 1 时，那么这个元素在集合内，反之则不在集合内`。这个方法的`缺点就是当检测的元素很多的时候可能有冲突，解决方法就是使用 k 个哈希 函数对应 k 个点，如果所有点都是 1 的话，那么元素在集合内，如果有 0 的话，元素则不再集合内。随着元素的插入，Bloom filter 中修改的值变多，出现误判的几率也随之变大`，当新来一个元素时，满足其在集合内的条件，即所有对应位都是 1 ，这样就可能有两种情况，一是这个元素就在集合内，没有发生误判；还有一种情况就是发生误判，出现了哈希碰撞，这个元素本不在集合内。


## 一个数组A[N]，包含取值为[1,N]的元素，请判断是否有重复元素
解法：
1、Sum(1…N)!=sum(A[0],A[N-1])则重复
2、hash记数法
3、排序后再判重


## 3*4的格子有几个矩形:
M*N网格中有横竖各M+1、N+1条直线，其中，任意各取两条都可以组成一个长方形。
C(4,2)*C(5,2)=6*10=60;
A(N,N)=N!
A(N,M)=N*(N-1)*…*(N-M+1)
C(N,M)=A(N,M)/A(M,M)




## 50个红球，50个篮球，2个袋子，一个袋子能装任意个球(0~100)。
现由你将这100个球，以一定方法装入这两个袋子。另找一个不明真相的路人，闭眼，随机从两个袋子中任意摸一个球。
要使得他摸出红球的概率最高，你应该如何分配这100个球。
答案：一个袋子一个红球，另一个袋子49个红球+50个蓝球
首先可能会列方程解，说明思路比较清晰。但方程比较难解，如果可以解出来就加分。如果解不出来，建议他通过思考解答
能首先将问题分解为两个袋子红球概率和最大，加分
首先优化一个袋子红球的概率(50个红球全在袋子中)，其次不损失多余的红球，即可得出答案
还需要通过迭代法验证答案的正确性


## 如何使用P、V操作来结局各种生产者-消费者问题？
PV操作由P操作原语和V操作原语组成（原语是不可中断的过程），对信号量进行操作，具体定义如下：
    P（S）：①将信号量S的值减1，即S=S-1；
           ②如果S0，则该进程继续执行；否则该进程置为等待状态，排入等待队列。
    V（S）：①将信号量S的值加1，即S=S+1；
           ②如果S>0，则该进程继续执行；否则释放队列中第一个等待信号量的进程。
PV操作的意义：我们用信号量及PV操作来实现进程的同步和互斥。PV操作属于进程的低级通信。
信号量（semaphore）的数据结构为一个值和一个指针，指针指向等待该信号量的下一个进程。信号量的值与相应资源的使用情况有关。当它的值大于0时，表示当前可用资源的数量；当它的值小于0时，其绝对值表示等待使用该资源的进程个数。注意，信号量的值仅能由PV操作来改变。

一般来说，信号量S>0时，S表示可用资源的数量。执行一次P操作意味着请求分配一个单位资源，因此S的值减1；当S<0时，表示已经没有可用资源，请求者必须等待别的进程释放该类资源，它才能运行下去。而执行一个V操作意味着释放一个单位资源，因此S的值加1；若S0，表示有某些进程正在等待该资源，因此要唤醒一个等待状态的进程，使之运行下去。
利用信号量和PV操作实现进程互斥：
	进程P1              进程P2           ……          进程Pn
	……                  ……                           ……
	P（S）；              P（S）；                         P（S）；
	临界区；             临界区；                        临界区；
	V（S）；              V（S）；                        V（S）；
	……                  ……            ……           ……

其中信号量S用于互斥，初值为1。
使用PV操作实现进程互斥时应该注意的是：
  （1）每个程序中用户实现互斥的P、V操作必须成对出现，先做P操作，进临界区，后做V操作，出临界区。若有多个分支，要认真检查其成对性。
  （2）P、V操作应分别紧靠临界区的头尾部，临界区的代码应尽可能短，不能有死循环。
  （3）互斥信号量的初值一般为1。

利用信号量和PV操作实现进程同步
PV操作是典型的同步机制之一。用一个信号量与一个消息联系起来，当信号量的值为0时，表示期望的消息尚未产生；当信号量的值非0时，表示期望的消息已经存在。用PV操作实现进程同步时，调用P操作测试消息是否到达，调用V操作发送消息。
使用PV操作实现进程同步时应该注意的是：
  （1）分析进程间的制约关系，确定信号量种类。在保持进程间有正确的同步关系情况下，哪个进程先执行，哪些进程后执行，彼此间通过什么资源（信号量）进行协调，从而明确要设置哪些信号量。
  （2）信号量的初值与相应资源的数量有关，也与P、V操作在程序代码中出现的位置有关。
  （3）同一信号量的P、V操作要成对出现，但它们分别在不同的进程代码中。

## 生产者-消费者问题
在多道程序环境下，进程同步是一个十分重要又令人感兴趣的问题，而生产者-消费者问题是其中一个有代表性的进程同步问题。下面我们给出了各种情况下的生产者-消费者问题，深入地分析和透彻地理解这个例子，对于全面解决操作系统内的同步、互斥问题将有很大帮助。
（1）一个生产者，一个消费者，公用一个缓冲区。
定义两个同步信号量：
	empty——表示缓冲区是否为空，初值为1。
   	full——表示缓冲区中是否为满，初值为0。
生产者进程
	while(TRUE){
		生产一个产品;
     	P(empty);
     	产品送往Buffer;
     	V(full);
	}
消费者进程
	while(True){
		P(full);
   		从Buffer取出一个产品;
   		V(empty);
   		消费该产品;
   	}
（2）一个生产者，一个消费者，公用n个环形缓冲区。
定义两个同步信号量：
	empty——表示缓冲区是否为空，初值为n。
	full——表示缓冲区中是否为满，初值为0。
	设缓冲区的编号为1～n-1，定义两个指针in和out，分别是生产者进程和消费者进程使用的指针，指向下一个可用的缓冲区。
生产者进程
	while(TRUE){
     	生产一个产品;
     	P(empty);
     	产品送往buffer（in）；
     	in=(in+1)mod n；
     	V(full);
	}
消费者进程
	while(TRUE){
 		P(full);
   		从buffer（out）中取出产品；
   		out=(out+1)mod n；
   		V(empty);
   		消费该产品;
   	}
（3）一组生产者，一组消费者，公用n个环形缓冲区
在这个问题中，不仅生产者与消费者之间要同步，而且各个生产者之间、各个消费者之间还必须互斥地访问缓冲区。
定义四个信号量：
empty——表示缓冲区是否为空，初值为n。
full——表示缓冲区中是否为满，初值为0。
mutex1——生产者之间的互斥信号量，初值为1。
mutex2——消费者之间的互斥信号量，初值为1。
	设缓冲区的编号为1～n-1，定义两个指针in和out，分别是生产者进程和消费者进程使用的指针，指向下一个可用的缓冲区。
生产者进程
while(TRUE){
     生产一个产品;
     P(empty);
     P(mutex1)；
     产品送往buffer（in）；
     in=(in+1)mod n；
     V(mutex1);
     V(full);
}
消费者进程
while(TRUE){
 P(full)
   P(mutex2)；
   从buffer（out）中取出产品；
   out=(out+1)mod n；
   V（mutex2）；
   V(empty);
   消费该产品;
   }
  需要注意的是无论在生产者进程中还是在消费者进程中，两个P操作的次序不能颠倒。应先执行同步信号量的P操作，然后再执行互斥信号量的P操作，否则可能造成进程死锁。

## 多信号量生产者-消费者问题
桌上有一空盘，允许存放一只水果。爸爸可向盘中放苹果，也可向盘中放桔子，儿子专等吃盘中的桔子，女儿专等吃盘中的苹果。规定当盘空时一次只能放一只水果供吃者取用，请用P、V原语实现爸爸、儿子、女儿三个并发进程的同步。
分析 在本题中，爸爸、儿子、女儿共用一个盘子，盘中一次只能放一个水果。当盘子为空时，爸爸可将一个水果放入果盘中。若放入果盘中的是桔子，则允许儿子吃，女儿必须等待；若放入果盘中的是苹果，则允许女儿吃，儿子必须等待。本题实际上是生产者-消费者问题的一种变形。这里，生产者放入缓冲区的产品有两类，消费者也有两类，每类消费者只消费其中固定的一类产品。
解：在本题中，应设置三个信号量S、So、Sa，信号量S表示盘子是否为空，其初值为l；信号量So表示盘中是否有桔子，其初值为0；信号量Sa表示盘中是否有苹果，其初值为0。同步描述如下：
	int S＝1;
	int Sa＝0;
	int So＝0;
      main()
      {
        cobegin
            father();      /*父亲进程*/
            son();        /*儿子进程*/
            daughter();    /*女儿进程*/
        coend
    ｝
    father()
    {
        while(1)
          {
            P(S);
            将水果放入盘中;
            if（放入的是桔子）V(So);
            else  V(Sa);
           }
     }
    son()
    {
        while(1)
          {
             P(So);
             从盘中取出桔子;
             V(S);
             吃桔子;
            ｝
    }
    daughter()
    {
         while(1)
            {
              P(Sa);
              从盘中取出苹果;
              V(S);
              吃苹果;
            ｝
｝


## 向单链表中满足条件的位置插入一个元素
通常是向有序单链表中插入一个新元素结点，使得插入后仍然有序。其插入过程为：
(1)为新元素动态分配结点并赋值；
(2)若单链表为空或者新元素小于表头结点的值，则应把新元素结点插入到表头并返
(3)从表头开始顺序查找新元素的插入位置，在查找过程中必须保留当前结点的前驱g指针，以便插入新结点时使用；
(4)在插入位置上完成插入操作，即把新结点插入到当前结点和其前驱结点之间。


## 54张扑克牌，一半红色一半黑色，随机取两张，一红一黑的概率
27/53


## 排序相关问题
1.基于比较的排序，其复杂度最佳是多少？
2.快排排序、归并排序，其复杂度是多少？
3.既然快排O(n^2) > O(nlogn), 为什么实际应用中，快排的表现经常优于归并排序？
4.存在O(n)级别的排序算法么？
答案：
1.O(nlogn)
2.快排O(n^2)，归并排序O(nlogn)
3.inplace、cache性能好
4.存在，非比较排序。如counting sort, radix sort。
（最基础的问题，必须正确。）


## 有1000瓶水，其中有一瓶有毒，小白鼠只要尝一点带毒的水24小时后就会死亡至少要多少只小白鼠才能在24小时鉴别出哪瓶水有毒。
给1000个瓶分别标上如下标签（10位长度）： 
0000000001 （第1瓶） 
0000000010 （第2瓶） 
0000000011 （第3瓶） 
...... 
1111101000 （第1000瓶） 
从编号最后1位是1的所有的瓶子里面取出1滴混在一起（比如从第一瓶，第三瓶，…里分别取出一滴混在一起）并标上记号为1。以此类推，从编号第一位是1的所有的瓶子里面取出1滴混在一起并标上记号为10。现在得到有10个编号的混合液，小白鼠排排站，分别标上10，9，…1号，并分别给它们灌上对应号码的混合液。24小时过去了，过来验尸： 
从左到右，死了的小白鼠贴上标签1，没死的贴上0，最后得到一个序号，把这个序号换成10进制的数字，就是有毒的那瓶水的编号。


## 输入一个单向链表，输出该链表中倒数第k个结点。链表的倒数第0个结点为链表的尾指针。
1.从头节点开始遍历链表，直到链表为节点，统计链表节点个数n。那么倒数第k个节点就是从头结点开始的第n-k-1个节点，则再从头结点开始遍历链表，直到第n-k-1个节点为止。这种思路需要遍历链表两次。
2.在遍历时维持两个指针，第一个指针从链表的头指针开始遍历，在第k-1步之前，第二个指针保持不动；在第k-1步开始，第二个指针也开始从链表的头指针开始遍历。由于两个指针的距离保持在k-1，当第一个（走在前面的）指针到达链表的尾结点时，第二个指针（走在后面的）指针正好是倒数第k个结点。这种思路只需要遍历链表一次。


## 一条线把平面分成两块，两条线把平面分成四块，如果任意两条线不平行，且没有3条线交在同一点，问100条线将平面分成多少块。
答案：5051
1条直线最多将平面分成2个部分；2条直线最多将平面分成4个部分；3条直线最多将平面分成7个部分；现在添上第4条直线．它与前面的3条直线最多有3个交点，这3个交点将第4条直线分成4段，其中每一段将原来所在平面部分一分为二，所以4条直线最多将平面分成7+4=11个部分． 
完全类似地，5条直线最多将平面分成11+5=16个部分；6条直线最多将平面分成16+6=22个部分；7条直线最多将平面分成22+7=29个部分；8条直线最多将平面分成29+8=37个部分． 
一般地，n条直线最多将平面分成2+2+3....+N=（N*N+N+2）/2


## RESTful架构风格理解
REST并不是一种具体的技术，也不是一种具体的规范，`REST其实是一种内涵非常丰富的架构风格`。它是为运行在互联网环境的分布式超媒体系统量身定制的。互联网环境与企业内网环境有非常大的差别，最主要的差别是两个方面：
（1）可伸缩性需求无法控制：并发访问量可能会暴涨，也可能会暴跌。
（2）安全性需求无法控制：无法控制客户端发来的请求的格式，很可能会是恶意的请求。

从架构风格的抽象高度来看，常见的分布式应用架构风格有三种：
（1）分布式对象（Distributed Objects，简称DO），架构实例有CORBA/RMI/EJB/DCOM/.NET Remoting等等；
（2）远程过程调用（Remote Procedure Call，简称RPC），架构实例有SOAP/XML-RPC/Hessian/Flash AMF/DWR等等；
（3）表述性状态转移（Representational State Transfer，简称REST），架构实例有HTTP/WebDAV；

REST是HTTP/1.1协议等Web规范的设计指导原则，`HTTP/1.1协议正是为实现REST风格的架构而设计的`。REST是所有Web应用都应该遵守的架构设计指导原则。当然，REST并不是法律，违反了REST的指导原则，仍然能够实现应用的功能。但是违反了REST的指导原则，会付出很多代价，特别是对于大流量的网站而言。
要深入理解REST，需要理解REST的五个关键词：
（1）资源（Resource）
资源是一种看待服务器的方式，即，`将服务器看作是由很多离散的资源组成`。每个资源是服务器上一个可命名的抽象概念。因为资源是一个抽象的概念，所以它不仅仅能代表服务器文件系统中的一个文件、数据库中的一张表等等具体的东西，可以将资源设计的要多抽象有多抽象，只要想象力允许而且客户端应用开发者能够理解。与面向对象设计类似，资源是以名词为核心来组织的，首先关注的是名词。一个资源可以由一个或多个URI来标识。URI既是资源的名称，也是资源在Web上的地址。对某个资源感兴趣的客户端应用，可以通过资源的URI与其进行交互。
（2）资源的表述（Representation）
资源的表述是一段对于资源在某个特定时刻的状态的描述。可以在客户端-服务器端之间转移（交换）。资源的表述可以有多种格式，例如HTML/XML/JSON/纯文本/图片/视频/音频等等。资源的表述格式可以通过协商机制来确定。请求-响应方向的表述通常使用不同的格式。
（3）状态转移（State Transfer）
状态转移（state transfer）与状态机中的状态迁移（state transition）的含义是不同的。状态转移说的是：在客户端和服务器端之间转移（transfer）代表资源状态的表述。通过转移和操作资源的表述，来间接实现操作资源的目的。
（4）统一接口（Uniform Interface）
REST要求，必须通过统一的接口来对资源执行各种操作。对于每个资源只能执行一组有限的操作。以HTTP/1.1协议为例，HTTP/1.1协议定义了一个操作资源的统一接口，主要包括以下内容：
1）7个HTTP方法：GET/POST/PUT/DELETE/PATCH/HEAD/OPTIONS
2）HTTP头信息（可自定义）
3）HTTP响应状态代码（可自定义）
4）一套标准的内容协商机制
5）一套标准的缓存机制
6）一套标准的客户端身份认证机制
（5）超文本驱动（Hypertext Driven）
“超文本驱动”又名“将超媒体作为应用状态的引擎”（Hypermedia As The Engine Of Application State，来自Fielding博士论文中的一句话，缩写为HATEOAS）。将Web应用看作是一个由很多状态（应用状态）组成的有限状态机。资源之间通过超链接相互关联，超链接既代表资源之间的关系，也代表可执行的状态迁移。在超媒体之中不仅仅包含数据，还包含了状态迁移的语义。以超媒体作为引擎，驱动Web应用的状态迁移。通过超媒体暴露出服务器所提供的资源，服务器提供了哪些资源是在运行时通过解析超媒体发现的，而不是事先定义的。从面向服务的角度看，超媒体定义了服务器所提供服务的协议。客户端应该依赖的是超媒体的状态迁移语义，而不应该对于是否存在某个URI或URI的某种特殊构造方式作出假设。一切都有可能变化，只有超媒体的状态迁移语义能够长期保持稳定。

REST风格的架构所具有的6个主要特征：
（1）面向资源（Resource Oriented）
（2）可寻址（Addressability）
（3）连通性（Connectedness）
（4）无状态（Statelessness）
（5）统一接口（Uniform Interface）
（6）超文本驱动（Hypertext Driven）
这6个特征是REST架构设计优秀程度的判断标准。其中，面向资源是REST最明显的特征，即，REST架构设计是以资源抽象为核心展开的。可寻址说的是：每一个资源在Web之上都有自己的地址。连通性说的是：应该尽量避免设计孤立的资源，除了设计资源本身，还需要设计资源之间的关联关系，并且通过超链接将资源关联起来。无状态、统一接口是REST的两种架构约束，超文本驱动是REST的一个关键词。


## 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
还是典型的TOP K算法，解决方案如下：
方案1：
  顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。
找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件。
对这10个文件进行归并排序（内排序与外排序相结合）。

方案2：
一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。

方案3：
与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。





















































