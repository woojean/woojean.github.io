《构建高性能Web站点》读书笔记

# 第1章 绪论
(略)

# 第2章 数据的网络传输
（通过一个构建铁路系统的故事复习TCP的基础知识，略）

## 带宽
数据发送的过程（数据从主机进入线路的过程）：
1.应用程序将要发送的数据写入进程的内存地址空间中；（运行时变量赋值）
2.应用程序向内核发出系统调用，内核将数据从用户态内存复制到由内核维护的内核缓冲区中。（内核缓冲区的大小是有限的，所有要发送的数据以队列形式进入，这些数据可能来自于多个进程）
3.内核通知网卡控制器前来取数据，同时CPU转而处理其他进程。网卡控制器根据网卡驱动信息得知对应的内核缓冲区的地址，并将要发送的数据复制到网卡的缓冲区中。在以上的数据复制过程中，数据始终按照连接两端设备的内部总线宽度来复制，比如在32位总线的主机系统中，网卡一般也使用32位总线宽度，那么从内核缓冲区到网卡缓冲区的数据复制过程中，任何时刻只能复制32位的比特信息。
4.网卡将缓冲区中的数据按位转换成不同的光电信号，然后将数据的每个位按照顺序依次发出。

带宽指数据的发送速度。具体取决于以下两个因素：
1.信号传输频率：即数据发送装置将二进制信号传送至线路的能力以及另一端的数据接收装置对二进制信号的接收能力，同时也包含线路对传输频率的支持程度。
2.数据传播介质的并行度，完全等价于计算机系统总线宽度的概念。比如将若干条光纤并行组成光缆，这样就可以在同一个横截面上同时传输多个信号。
所以，要提高计算机总线的带宽，包括提高总线频率和总线宽度两种方法。

网络通信相比于计算机内部的数据传输而言有一个很大的不同，其传输距离比较大，所以信号在传播介质中会衰减，为此需要借助中继器来延续信号，而每次中继器转发信号又会消耗一些发送时间。

## 共享和独享
运营商会在所有的基础交换节点上设置关卡，即限制数据从用户主机流入路由器转发队列的速度。
如果某台主机使用了独享10M带宽，且路由器的出口带宽为100M，那么`交换机的设置应该保证来自该广播域内的其他主机的数据发送速度总和不超过90Mbit/s`，以此保证该主机任何时刻都可以以10Mbit/s的速度发送数据。即该主机独享路由器的一部分出口带宽。
如果是共享100M带宽，则交换机不保证主机的出口带宽能达到100M。

## 响应时间
响应时间即数据从服务器开始发送直到完全到达用户PC的这段时间。
`响应时间 = 发送时间 + 传播时间 + 处理时间`
发送时间 = 数据量/带宽，当存在多个交换节点时，也应该包含每个节点的转发时间
传播时间 = 传播距离/传播速度
处理时间指数据在交换节点中为存储转发而进行一些必要处理所花费的时间，主要是数据在缓冲区队列中排队所花费的时间。

`下载速度 = 数据量字节数/响应时间`

在实际的互联网中，瓶颈也可能出现在各互联网运营商之间的网络互连上。


# 第3章 服务器并发处理能力

## 吞吐率
吞吐率指服务器单位时间内处理的请求数，是在一定并发用户数的情况下服务器处理请求能力的量化体现。
通常所讲的最大并发数是有一定的利益前提的，即服务器和用户双方所期待的最大收益的平衡。得出最大并发数的意义在于了解服务器的承载能力。
需要注意的是，所谓的最大并发数并非指和真实用户的一一对应关系，因为一个真实用户可能会给服务器带来多个并发用户数压力。
从Web服务器的角度来看，实际并发用户数也可以理解为Web服务器当前维护的、代表不同用户的文件描述符总数，Web服务器一般会限制同时服务的最多用户数，当实际并发用户数大于服务器所维护的文件描述符总数时，多出来的用户请求将在服务器内核的数据接收缓冲区中等待处理。

需要注意区别用户平均请求等待时间和服务器平均请求处理时间。当只有一个用户时，可以近似地认为用户平均请求等待时间等于服务器的平均请求处理时间。当并发用户数增加时，Web服务器会通过多个执行流来同时处理多个并发用户的请求（轮流交替使用时间片），所以每个执行流花费的时间都被拉长，因此用户的平均等待时间必然增加，而对于服务器而言，如果并发策略得当，每个请求的平均处理时间可能减少。

## 进程
进程是CPU多执行流的实现方式。多进程的好处不仅仅在于对CPU时间的轮流使用，还在于对CPU计算和I/O操作进行了很好的重叠利用。
进程的调度由内核进行，从内核的观点看，进程就是担当分配系统资源的实体，也可以理解为记录程序实例当前运行到什么程度的一组数据。
每个进程都有自己的独立内存地址空间和生命周期，当子进程被父进程创建后便将父进程地址空间的所有数据复制到自己的地址空间，完全继承父进程的所有上下文信息。
进程的创建使用fork()系统调用。

## 轻量级进程
由于进程之间相对独立，各自维护其庞大的地址空间和上下文信息，所以采用大量进程的Web服务器在处理大量并发请求时内存的大量消耗有时候会成为性能提升的制约因素。但是，进程的优越性也恰恰体现在其相互独立所带来的稳定性和健壮性。
Linux2.0之后提供对轻量级进程的支持，由新的系统调用clone()来创建，由内核直接管理，像普通的进程一样独立存在，拥有自己的进程描述符。但是这些进程已经允许共享一些资源，比如地址空间、打开的文件等，从而减少了内存的开销。当然，其上下文切换的开销在所难免。

## 线程
Linux线程的接口定义为pthread，它有多种实现，有些只是在普通的进程中由用户态通过一些库函数模拟实现的多执行流，这种情况下多线程的管理完全在用户态完成，线程的切换开销相比于进程和轻量级进程要少，但是在多处理器的服务器中表现较差。另一种实现是内核级的线程库，通过clone()来创建线程，即每个线程实际上就是一个轻量级进程，这使得线程完全由内核的进程调度器来管理，对多处理器服务器支持较好，但线程切换的开销也比用户态线程多。

## 进程调度器
内核中的进程调度器维护着各种状态的进程队列，其中包括所有可运行进程的队列称为运行队列。如果运行队列中有不止一个进程，进程调度器的一项主要工作就是决定下一个运行的进程，这项工作基于每个进程的进程优先级进行。
进程优先级除了可以由进程自己决定，进程调度器在进程运行时也可以动态调整它们的优先级。Linux对进程的动态调整体现在进程的nice属性中，在top命令中，进程的优先级属性为PR，优先级的动态调整值用NI表示。
PR所代表的值其实就是进程调度器分配给进程的时间片长度，单位是时钟个数。

## 系统负载
通过查看/proc/loadavg可以了解到运行队列的情况：
```
cat /proc/loadavg
1.63 0.48 0.21 10/200 17145
```
其中：
1.63 0.48 0.21是不同时间内（最近1分钟、5分钟、15分钟）的系统负载，是单位时间内运行队列中就绪等待的进程数的平均值，如果值为0，说明每个进程只要就绪后就可以马上获得CPU，无需等待，这时系统的响应速度最快。
10/200中的10代表此时运行队列中的进程个数，而200代表此时的进程总数。
17145代表最后创建的一个进程ID。
也可以通过top命令或者w命令来获得系统负载，它们其实仍然来自于/proc/loadavg。

## 进程切换
一个进程被挂起的本质就是将它在CPU寄存器中的数据拿出来暂存在内核态堆栈中，而一个进程恢复工作的本质就是将它的数据重新装入CPU寄存器中，这些移出和装入的数据称为进程的硬件上下文。
可以使用Nmon工具监视系统的平均上下文切换情况。

## IOWait
指CPU空闲并且等待I/O操作完成的时间比例。
（详略）

## 锁竞争
除非我们自己编写Web服务器，否则不需要太担心目前流行的Web服务器的锁设计本身。
（详略）

## 系统调用
Linux为进程设计了两种运行级别：用户态和内核态。进程通过系统调用实现在两种状态间的切换。系统调用的开销比较昂贵。

应该优化Web服务器的配置，去掉不必要的系统调用，如：
Apache支持通过.htaccess文件来为htdocs下的各个目录进行局部的参数配置，但它有一定的副作用。当将httpd.conf的AllowOverride设置为All时，使用strace来跟踪Apache的一个子进程可以发现，在某次请求处理中的一系列系统调用中有很多open系统调用，目的在于检查被访问的文件路径中各级目录下是否存在.htaccess文件。

## 内存分配
Nginx的内存分配策略优于Apache，它使用多线程来处理请求，使得多个线程之间可以共享内存资源。此外，使用分阶段的内存分配策略，按需分配，及时释放，使得内存使用量保持在很小的数量范围。
Nginx声称维持10000个非活跃HTTP持久链接只需要2.5M内存。

## 持久链接
持久链接也称长连接，它本身是TCP通信的一种普通方式，即在一次TCP链接中持续发送多份数据而不断开连接，与它相反的方式称为短连接，短连接每次发送数据都需要建立新的TCP连接。
HTTP长连接的实现需要浏览器和Web服务器的共同协作，一方面浏览器需要保持一个TCP连接并重复利用（表现在HTTP头中的Connection:Keep-Alive），不断地发送多个请求，另一方面，服务器不能过早地主动关闭连接。
浏览器和Web服务器各自的长连接超时时间的设置可能不一致，所以在实际运行中是以最短的超时时间为准。

## I/O模型
一开始磁盘和内存之间的数据传输是由CPU控制的，即数据需要经过CPU存储转发，这种方式称为PIO。后来有了DMA（Direct Memory Access），可以不经过CPU而直接进行磁盘和内存的数据交换。CPU只需要向DMA控制器下达指令，让其处理数据传送，DMA控制器使用系统总线传输数据，完毕后再通知CPU。

## 同步阻塞I/O
同步阻塞I/O指当进程调用某些涉及I/O操作的系统调用或函数库时（accept()、send()、recv()等），进程暂停，等待I/O操作完成后再继续运行。

## 同步非阻塞I/O
同步非阻塞I/O的调用不会等待数据的就绪，如果数据不可读或者不可写，它会立即告诉进程。相比于阻塞I/O这种非阻塞I/O结合反复轮询来尝试数据是否就绪，最大的好处是便于在一个进程里同时处理多个I/O操作。缺点在于会花费大量的CPU时间，使得进程处于忙碌等待状态。
非阻塞I/O一般只对网络I/O有效，比如在socket的选项中设置O_NONBLOCK。

## 多路I/O就绪通知
多路I/O就绪通知允许进程通过一种方法来同时监视所有文件描述符，并可以快速获得所有就绪的文件描述符，然后只对这些文件描述符进行数据访问。
I/O就绪通知只是有助于快速获得就绪的文件描述符，当得知数据就绪后，就访问数据本身而言，仍然需要选择阻塞或者非阻塞的访问方式。

select
监视包含多个文件描述符的数组，当select()返回后，该数组中就绪的文件描述符会被修改标志位使得进程可以获得这些文件描述符从而进行后续的读写操作。有点在于几乎所有平台都支持，缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上为1024，假如维持的连接已达上限，新的连接请求将被拒绝。此外，select所维护的数据结构复制开销较大，对于非活跃状态的TCP连接也会进行线性扫描等，也是缺陷。

poll
本质上和select没有太大区别（实现者不同），包含大量文件描述符的数组被整体复制于用户态和内核态的地址空间，而不论这些文件描述符是否就绪，但是poll没有最大文件描述符的数量限制。

SIGIO
通过实时信号来通知，不同于select/poll，对于变为就绪状态的文件描述符，SIGIO只通知一遍。所以存在事件丢失的情况，需要采用其他方法弥补。

/dev/poll
使用虚拟的/dev/poll设备，可以将要监视的文件描述符数组写入这个设备，然后通过ioctl()来等待事件通知，当ioctl()返回就绪的文件描述符后，可以从/dev/poll中读取所有就绪的文件描述符数组，节省了扫描所有文件描述符的开销。

/dev/epoll
在/dev/poll的基础上增加了内存映射（mmap）的技术

epoll
Linux2.6中由内核直接支持的实现方法，公认的Linux2.6下性能最好的多路I/O就绪通知方法。
epoll的本质改进在于其基于事件的就绪通知方式，事先通过epoll_ctl()来注册每一个文件描述符，一旦某个文件描述符就绪时，内核会采用类似callback的回调机制迅速激活这个文件描述符，当进程调用epoll_wait()时得到通知，获得就绪的文件描述符的数量的值，然后只需要去epoll指定的数组中依次取得相应数量的文件描述符即可。

kqueue
性能和epoll接近，但很多平台不支持。

## 内存映射
内存映射是一种访问磁盘文件的特殊方式，可以将内存中某块地址空间和指定的磁盘文件相关联，从而把对这块内存的访问转换成对磁盘文件的访问。内存映射无需使用read()和write()等系统调用来访问文件，而是通过mmap()系统调用来建立内存和磁盘文件的关联，然后像访问内存一样自由地访问文件。

## 直接I/O
在Linux2.6中，内存映射和直接访问文件没有本质上的差异，因为数据从进程用户态内存空间到磁盘都要经过两次复制：磁盘到内核缓冲区、内核缓冲区到用户态内存空间。
对于一些复杂的应用，比如数据库服务器，为了提高性能希望绕过内核缓冲区由自己在用户态空间实现并管理I/O缓冲区以支持独特的查询机制。Linux提供了对这种需求的支持，即在open()系统调用中添加参数O_DIRECT，有效避免CPU和内存的多余时间开销。

## sendfile
通常向Web服务器请求静态文件的过程是：磁盘文件的数据经过内核缓冲区到达用户内存空间，然后被送到网卡对应的内核缓冲区，接着被送进网卡并发送。数据从内核出去，没有经过任何变化，又回到了内核，因此浪费时间。
sendfile()系统调用可以将磁盘文件的特定部分直接送到代表客户端的socket描述符中，从而加快静态文件的请求速度，同时减少CPU和内存的开销。
Apache对于较小的静态文件选择使用内存映射来读取，对于较大的静态文件使用sendfile来传送文件。

## 异步I/O
同步和异步、阻塞和非阻塞修饰的是不同的对象。
阻塞和非阻塞指当进程访问的数据未就绪时，进程是直接返回还是继续等待。
同步和异步指访问数据的机制，同步指主动请求并等待I/O操作完毕，在数据就绪后，读写时必须阻塞。异步指主动请求数据后便可以继续处理其他任务，随后等待I/O操作完毕的通知，使得进程在数据读写时也不发生阻塞。

## 服务器并发策略
从本质上讲所有到达Web服务器的请求都封装在IP包中，位于网卡的接收缓冲区内。Web服务器软件要做的事情就是不断地读取这些请求，进行处理，再将结果写到发送缓冲区。这个过程中涉及很多I/O操作和CPU计算，并发策略的目的就是让I/O操作和CPU计算尽量重叠进行，让CPU在I/O等待时不要空闲，同时在I/O调度上尽量花费最少的时间。

1.一个进程处理一个连接，非阻塞I/O
fork()模式、prefork()模式；
并发连接数有限，但是稳定性和兼容性较好。

2.一个线程处理一个连接，非阻塞I/O
比如Apache的worker模型，这里的线程实际是轻量级进程，实际并不比prefork有太大优势。

3.一个进程处理多个连接，非阻塞I/O
这种模式下，多路I/O就绪通知的性能成为关键。
这种处理多个连接的进程称为work进程，通常数量可配，比如在Nginx中：worker_processes 2;

4.一个线程处理多个连接，异步I/O
对于磁盘文件的操作，设置文件描述符为非阻塞没有任何意义：如果需要读取的数据不在磁盘缓冲区，磁盘便开始动用物理设备来读取数据，这时整个进程的其他工作必须等待。目前几乎没有Web服务器支持基于Linux AIO的工作方式。


# 第4章 动态内容缓存

## Smarty缓存
```
require '../libsmarty/Smarty.class.php';
$this->smarty = new Smarty();
$this->smarty->caching = true;

$this->template_page = 'place_posts.html';
$this->cache_id = $this->marker_id;

if( $this->smarty->is_cached($this->template_page, $this->cache_id) ){
   $this->smarty->display( $this->template_page, $this->cache_id ); 
   exit(0);
}
do_some_db_query();
...
$this->smarty->display( $this->template_page, $this->cache_id ); 
```

缓存保存、查找、过期检查等，主要是Smarty的API，略。

## APC
Smarty的存储基于磁盘，而APC基于内存。

```
$this->key = $this->template_page . $this->cache_id;
$html = apc_fetch( $this->key );
if( $html !== false ){
    echo $html;
    exit(0);
}
do_some_db_query();
$html = $this->smarty->fetch($this->template_page , $this->cache_id);
apc_add($this->key, $html, $this->smarty->cache_lifetime);
echo $html;
```

## XCache
和APC类似，略。

## memcached
memcached支持将缓存存储在独立的缓存服务器中。
有Redis了，略。

## 局部无缓存
基本实现方式都是自定义一组标签，然后在模板中将局部无缓存的内容用标签包含，比如Smarty：
```
function smarty_block_dynamic( $params, $content, &$smarty){
    return $content;
}
$this->smarty->register_block('dynamic', 'smarty_block_dynamic', false);

{dynamic}
$user->user_nick;
{dynamic}
```
详略。

## 静态化内容
一般基于CMS来管理静态化内容。

## 服务器端包含
SSI（服务器端包含）技术实现各个局部页面的独立更新，比如Apache中的mod_include模块：
```
AddType text/html .shtml
AddOutputFilter INCLUDES .shtml
```
一旦网页支持SSI（按如上配置，即后缀为.shtml），那么每次请求的时候服务器必须要通读网页内容查找include标签，这需要大量的CPU开销。

SSI语法略。


# 第5章 动态脚本加速






































